import sys
import re
import json
import time
import mmap
import os
import asyncio
import hashlib
import argparse
from pathlib import Path
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import lru_cache
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn, TaskProgressColumn
from rich.panel import Panel
from rich import print as rprint
from .utils.duplicate_finder import DuplicateFinder
from .utils.web_scanner import download_and_scan_website
from difflib import SequenceMatcher
from typing import Dict, List, Set, Optional, Tuple

console = Console()

BANNER = r"""
 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó 
 ‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó
 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ïë       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë
 ‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù  ‚ñà‚ñà‚ïë     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù     ‚ñà‚ñà‚ïë       ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë
 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïë       ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù
 ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù   ‚ïö‚ïê‚ïù       ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù 
                                                                                             
                                                / \__
                                               (    @\___
                                               /         O
                                              /   (_____/
                                            /_____/   U

                        [bold cyan]A powerful tool for sniffing out secrets in your codebase[/bold cyan]
"""

COLORS = {
    "Private Key PEM": "red bold",
    "Password": "red bold", 
    "Credit Card": "red bold",
    "API Key": "red",
    "JWT Token": "green",
    "Email": "yellow",
    "Phone": "cyan",
    "URL": "blue",
    "Default": "white",
}

SUPPORTED_EXTENSIONS = {
    ".py", ".js", ".ts", ".java", ".c", ".cpp", ".rb", ".php", ".cs",
    ".go", ".rs", ".json", ".yaml", ".yml", ".env", ".log", ".txt",
    ".html", ".xml", ".sql", ".md", ".conf", ".properties"
}

EXCLUDE_DIRS = {".git", "__pycache__", "venv", "node_modules", ".vscode"}

OUTPUT_DIR = Path('output')
OUTPUT_DIR.mkdir(exist_ok=True)

PATTERNS = None

def display_results_optimized(results):
    console.print("\n" + "=" * 60)
    console.print("[bold cyan][üîç] –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–Ø[/bold cyan]", justify="center")
    console.print("=" * 60 + "\n")
    if not results:
        console.print("[green][‚úì] –ù–µ –Ω–∞–π–¥–µ–Ω–æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö[/green]")
        return
    severity_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
    results_by_severity = {}
    for item in results:
        severity = item.get("severity", "medium")
        if severity not in results_by_severity:
            results_by_severity[severity] = {}
        item_type = item["type"]
        if item_type not in results_by_severity[severity]:
            results_by_severity[severity][item_type] = []
        results_by_severity[severity][item_type].append(item)
    for severity in sorted(results_by_severity.keys(), key=lambda x: severity_order.get(x, 99)):
        severity_color = {
            "critical": "red bold",
            "high": "red",
            "medium": "yellow",
            "low": "blue"
        }.get(severity, "white")
        console.print(f"\n[{severity_color}]‚ïê‚ïê‚ïê {severity.upper()} SEVERITY ‚ïê‚ïê‚ïê[/{severity_color}]")
        for item_type, items in results_by_severity[severity].items():
            color = COLORS.get(item_type, COLORS["Default"])
            console.print(f"\n[bold][{color}]{item_type}[/][/bold] ({len(items)} –Ω–∞–π–¥–µ–Ω–æ)")
            table = Table(show_header=True, header_style="bold magenta")
            table.add_column("File", style="cyan")
            table.add_column("Line", justify="right", style="green")
            table.add_column("Snippet", style="white")
            for item in items:
                table.add_row(
                    item["file"],
                    str(item["line"]),
                    item["snippet"]
                )
            console.print(table)

class OptimizedScanner:
    def __init__(self, custom_domains=None, max_workers=None, cache_dir=None, search_term=None):
        global PATTERNS
        if PATTERNS is None:
            raise ValueError("PATTERNS –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ main_async() –±—ã–ª –≤—ã–∑–≤–∞–Ω –ø–µ—Ä–≤—ã–º.")
        self.compiled_patterns = self._compile_patterns()
        self.custom_domain_pattern = self._compile_custom_domains(custom_domains)
        self.file_cache = {}
        self.max_workers = max_workers or (os.cpu_count() * 2)
        self.cache_dir = cache_dir
        self.search_term = search_term
        if cache_dir:
            os.makedirs(cache_dir, exist_ok=True)
        self.process_pool = ProcessPoolExecutor(max_workers=self.max_workers)
    def _compile_patterns(self):
        compiled = {}
        for name, pattern in PATTERNS.items():
            if isinstance(pattern, str):
                flags = re.IGNORECASE | re.MULTILINE
                compiled[name] = re.compile(pattern, flags)
            else:
                compiled[name] = pattern
        return compiled
    def _compile_custom_domains(self, domains):
        if not domains:
            return None
        escaped = "|".join(re.escape(d.strip()) for d in domains if d.strip())
        if not escaped:
            return None
        pattern = rf"https?://(?:[\w-]+\.)*(?:{escaped})(?::\d+)?(?:/\S*)?"
        return re.compile(pattern, re.IGNORECASE | re.MULTILINE)
    def _get_file_hash(self, file_path):
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception:
            return None
    def _get_cache_path(self, file_path):
        if not self.cache_dir:
            return None
        file_hash = self._get_file_hash(file_path)
        if not file_hash:
            return None
        return os.path.join(self.cache_dir, f"{file_hash}.json")
    def _load_from_cache(self, file_path):
        cache_path = self._get_cache_path(file_path)
        if not cache_path or not os.path.exists(cache_path):
            return None
        try:
            with open(cache_path, 'r') as f:
                return json.load(f)
        except Exception:
            return None
    def _save_to_cache(self, file_path, results):
        cache_path = self._get_cache_path(file_path)
        if not cache_path:
            return
        try:
            with open(cache_path, 'w') as f:
                json.dump(results, f)
        except Exception:
            pass
    @lru_cache(maxsize=1000)
    def _should_skip_file(self, file_path_str):
        path = Path(file_path_str)
        try:
            if path.stat().st_size > 50 * 1024 * 1024:
                return True
        except OSError:
            return True
        return any(excl in path.parts for excl in EXCLUDE_DIRS)
    async def _read_file_async(self, file_path):
        try:
            file_size = file_path.stat().st_size
            with file_path.open('rb') as f:
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                    if file_size > 10 * 1024 * 1024:
                        buffer_size = 1024 * 1024
                        content = bytearray()
                        while True:
                            chunk = mm.read(buffer_size)
                            if not chunk:
                                break
                            content.extend(chunk)
                            await asyncio.sleep(0)
                        return content.decode('utf-8', errors='ignore')
                    else:
                        return mm.read().decode('utf-8', errors='ignore')
        except Exception as e:
            console.print(f"[yellow][WARNING] Cannot read {file_path}: {e}[/yellow]")
            return ""
    def _find_line_number(self, content, position):
        return content[:position].count('\n') + 1
    async def analyze_file_async(self, file_path, base_path, decode_unicode=False):
        if self._should_skip_file(str(file_path)):
            return []
        cached_results = self._load_from_cache(file_path)
        if cached_results is not None:
            return cached_results
        try:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ñ–∞–π–ª –ø–µ—Ä–µ–¥ –∞–Ω–∞–ª–∏–∑–æ–º, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –æ–ø—Ü–∏—è
            if decode_unicode:
                decode_file(str(file_path))
                
            content = await self._read_file_async(file_path)
            if not content:
                return []
            rel_path = str(file_path.relative_to(base_path))
            findings = []
            seen_hashes = set()
            if self.search_term:
                for match in re.finditer(re.escape(self.search_term), content):
                    line_num = self._find_line_number(content, match.start())
                    findings.append({
                        'file': rel_path,
                        'line': line_num,
                        'type': 'User Search',
                        'severity': 'info',
                        'snippet': content[match.start():match.end()].strip(),
                        'hash': hashlib.md5(content[match.start():match.end()].encode()).hexdigest()
                    })
            else:
                for pattern_name, pattern in self.compiled_patterns.items():
                    for match in pattern.finditer(content):
                        line_num = self._find_line_number(content, match.start())
                        snippet = match.group(0)[:100]
                        finding_hash = hash((pattern_name, rel_path, line_num, snippet))
                        if finding_hash not in seen_hashes:
                            seen_hashes.add(finding_hash)
                            findings.append({
                                "type": pattern_name,
                                "file": rel_path,
                                "line": line_num,
                                "snippet": snippet,
                                "severity": self._get_severity(pattern_name)
                            })
                if self.custom_domain_pattern:
                    for match in self.custom_domain_pattern.finditer(content):
                        line_num = self._find_line_number(content, match.start())
                        snippet = match.group(0)[:100]
                        finding_hash = hash(("Custom Domain URL", rel_path, line_num, snippet))
                        if finding_hash not in seen_hashes:
                            seen_hashes.add(finding_hash)
                            findings.append({
                                "type": "Custom Domain URL",
                                "file": rel_path,
                                "line": line_num,
                                "snippet": snippet,
                                "severity": "medium"
                            })
            self._save_to_cache(file_path, findings)
            return findings
        except Exception as e:
            console.print(f"[red][ERROR] Error processing file: {e}[/red]")
            return []
    def _get_severity(self, pattern_name):
        critical = ["Private Key PEM", "Password", "Credit Card", "API Key"]
        high = ["JWT Token", "Certificate", "Bank Account"]
        name_lower = pattern_name.lower()
        if any(crit.lower() in name_lower for crit in critical):
            return "critical"
        elif any(high_item.lower() in name_lower for high_item in high):
            return "high"
        else:
            return "medium"

async def scan_directory_async(path, extensions=None, decode_unicode=False):
    target_path = Path(path).resolve()
    if target_path.is_file():
        return [target_path]
    if not target_path.exists():
        console.print(f"[red][ERROR] Path does not exist: {target_path}[/red]")
        return []
    files = []
    extensions = extensions or SUPPORTED_EXTENSIONS
    async def scan_dir(dir_path):
        try:
            with os.scandir(dir_path) as entries:
                for entry in entries:
                    if entry.is_dir():
                        if entry.name not in EXCLUDE_DIRS:
                            await scan_dir(entry.path)
                    elif entry.is_file():
                        if Path(entry.path).suffix in extensions:
                            files.append(Path(entry.path))
                            await asyncio.sleep(0)
        except PermissionError:
            console.print(f"[yellow][WARNING] Permission denied: {dir_path}[/yellow]")
        except Exception as e:
            console.print(f"[yellow][WARNING] Error scanning {dir_path}: {e}[/yellow]")
    await scan_dir(target_path)
    return files

def decode_file(path: str) -> None:
    """
    –°—á–∏—Ç—ã–≤–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞, –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç unicode-—ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ—Ç —Ñ–∞–π–ª.
    """
    try:
        # –ß—Ç–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()

        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ \uXXXX –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        try:
            decoded = content.encode('utf-8').decode('unicode-escape')
        except UnicodeDecodeError:
            # Fallback for files that can't be decoded as unicode-escape
            decoded = content

        # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—å —Ñ–∞–π–ª–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º
        with open(path, 'w', encoding='utf-8') as f:
            f.write(decoded)
            
        console.print(f"[green]‚úì –§–∞–π–ª '{path}' —É—Å–ø–µ—à–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω[/green]")
    except Exception as e:
        console.print(f"[red]‚úó –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞ '{path}': {e}[/red]")

def parse_arguments():
    parser = argparse.ArgumentParser(description='–°–∫–∞–Ω–µ—Ä —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('-t', '--target',
                      help='–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏–ª–∏ —Ñ–∞–π–ª—É –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è)')
    parser.add_argument('-d', '--domains', 
                      help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º–∏ –¥–æ–º–µ–Ω–∞–º–∏ –∏–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å –∑–∞–ø—è—Ç—ã–º–∏')
    parser.add_argument('-b', '--big-patterns', action='store_true', 
                      help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (sensitive_patterns_big.py)')
    parser.add_argument('-c', '--cache', 
                      help='–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è')
    parser.add_argument('-s', '--search',
                      help='–ü–æ–∏—Å–∫ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∞–π–ª–∞—Ö')
    parser.add_argument('-ud', '--decode-unicode', action='store_true',
                      help='–î–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ñ–∞–π–ª–∞—Ö –ø–µ—Ä–µ–¥ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º')
    parser.add_argument('-u', '--url', 
                      help='URL –≤–µ–±-—Å–∞–π—Ç–∞ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (—Å–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª—ã –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Ö)')
    parser.add_argument('--web-output', default='web_files',
                      help='–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫–∞—á–∞–Ω–Ω—ã—Ö –≤–µ–±-—Ñ–∞–π–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: web_files)')
    parser.add_argument('--web-depth', type=int, default=3,
                      help='–ì–ª—É–±–∏–Ω–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è –≤–µ–±-—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 3)')
    parser.add_argument('--web-delay', type=float, default=0.1,
                      help='–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 0.1)')
    parser.add_argument('--web-max-size', type=int, default=10 * 1024 * 1024,
                      help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –≤ –±–∞–π—Ç–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10MB)')
    parser.add_argument('--no-web-follow-redirects', action='store_true',
                      help='–û—Ç–∫–ª—é—á–∏—Ç—å —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º –ø—Ä–∏ –≤–µ–±-—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏')
    parser.add_argument('--no-web-respect-robots', action='store_true',
                      help='–û—Ç–∫–ª—é—á–∏—Ç—å —Å–æ–±–ª—é–¥–µ–Ω–∏–µ robots.txt –ø—Ä–∏ –≤–µ–±-—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏')
    parser.add_argument('--update', action='store_true',
                      help='–û–±–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏ –≤–µ—Ä—Å–∏—é –ø—Ä–æ–µ–∫—Ç–∞')
    return parser.parse_args()

async def main_async():
    start_time = time.perf_counter()
    args = parse_arguments()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
    if args.update:
        from .utils.updater import SecretHoundUpdater
        updater = SecretHoundUpdater()
        success = updater.run_full_update()
        sys.exit(0 if success else 1)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É–∫–∞–∑–∞–Ω –ª–∏–±–æ target, –ª–∏–±–æ url
    if not args.target and not args.url:
        console.print("[red][ERROR] –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–∫–∞–∑–∞—Ç—å –ª–∏–±–æ -t/--target –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –ª–∏–±–æ -u/--url –¥–ª—è –≤–µ–±-—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è[/red]")
        sys.exit(1)
    global PATTERNS
    try:
        if args.big_patterns:
            from .utils.sensitive_patterns_big import PATTERNS as BIG_PATTERNS
            PATTERNS = BIG_PATTERNS
            console.print("[cyan]–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤[/cyan]")
        else:
            from .utils.sensitive_patterns import PATTERNS as STD_PATTERNS
            PATTERNS = STD_PATTERNS
            console.print("[cyan]–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –Ω–∞–±–æ—Ä –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤[/cyan]")
    except ImportError as e:
        console.print(f"[red][ERROR] –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}[/red]")
        sys.exit(1)
    custom_domains = None
    if args.domains:
        if os.path.isfile(args.domains):
            try:
                with open(args.domains, 'r', encoding='utf-8') as f:
                    custom_domains = [line.strip() for line in f if line.strip()]
                console.print(f"[cyan]–ó–∞–≥—Ä—É–∂–µ–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞: {args.domains}[/cyan]")
            except Exception as e:
                console.print(f"[red][ERROR] –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª —Å –¥–æ–º–µ–Ω–∞–º–∏: {e}[/red]")
        else:
            custom_domains = [d.strip() for d in args.domains.split(',') if d.strip()]
            if custom_domains:
                console.print(f"[cyan]–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –¥–æ–º–µ–Ω—ã: {', '.join(custom_domains)}[/cyan]")
            else:
                console.print(f"[yellow][WARN] –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å –¥–æ–º–µ–Ω—ã –∏–∑ —Å—Ç—Ä–æ–∫–∏: {args.domains}[/yellow]")
    scanner = OptimizedScanner(custom_domains, cache_dir=args.cache, search_term=args.search)
    if args.search:
        console.print(f"[cyan]–ü–æ–∏—Å–∫ —Å—Ç—Ä–æ–∫–∏: {args.search}[/cyan]")
    if args.decode_unicode:
        console.print("[cyan]–í–∫–ª—é—á–µ–Ω–æ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ Unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π[/cyan]")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∞–π–ª—ã –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
    if args.url:
        # –í–µ–±-—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        console.print(f"[cyan]üåê –í–µ–±-—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {args.url}[/cyan]")
        console.print(f"[cyan]üìä –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–µ–±-—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è:[/cyan]")
        console.print(f"[cyan]   ‚Ä¢ –ì–ª—É–±–∏–Ω–∞: {args.web_depth}[/cyan]")
        console.print(f"[cyan]   ‚Ä¢ –ó–∞–¥–µ—Ä–∂–∫–∞: {args.web_delay} —Å–µ–∫[/cyan]")
        console.print(f"[cyan]   ‚Ä¢ –ú–∞–∫—Å. —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {args.web_max_size // (1024*1024)}MB[/cyan]")
        console.print(f"[cyan]   ‚Ä¢ –°–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ä–µ–¥–∏—Ä–µ–∫—Ç–∞–º: {'–ù–µ—Ç' if args.no_web_follow_redirects else '–î–∞'}[/cyan]")
        console.print(f"[cyan]   ‚Ä¢ –°–æ–±–ª—é–¥–µ–Ω–∏–µ robots.txt: {'–ù–µ—Ç' if args.no_web_respect_robots else '–î–∞'}[/cyan]")
        
        web_output_dir = Path(args.web_output)
        files = await download_and_scan_website(
            url=args.url,
            output_dir=web_output_dir,
            max_depth=args.web_depth,
            max_file_size=args.web_max_size,
            follow_redirects=not args.no_web_follow_redirects,
            respect_robots_txt=not args.no_web_respect_robots,
            delay_between_requests=args.web_delay
        )
        if not files:
            console.print("[yellow]–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª—ã —Å –≤–µ–±-—Å–∞–π—Ç–∞[/yellow]")
            sys.exit(0)
    else:
        # –õ–æ–∫–∞–ª—å–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        files = await scan_directory_async(args.target, decode_unicode=args.decode_unicode)
        if not files:
            console.print("[yellow]–ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è[/yellow]")
            sys.exit(0)
    results = []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å –¥–ª—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
    if args.url:
        base_path = Path(args.web_output).resolve()
    else:
        base_path = Path(args.target).resolve()
    
    # –î–ª—è –≤–µ–±-—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –ø—É—Ç–∏
    if args.url:
        files = [Path(file).resolve() for file in files]
    
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
        TimeRemainingColumn(),
        console=console
    ) as progress:
        task = progress.add_task("[cyan]Scanning files...", total=len(files))
        tasks = [scanner.analyze_file_async(file, base_path, args.decode_unicode) for file in files]
        for completed_task in asyncio.as_completed(tasks):
            try:
                file_results = await completed_task
                results.extend(file_results)
            except Exception as e:
                console.print(f"[red][ERROR] Error processing file: {e}[/red]")
            finally:
                progress.update(task, advance=1)
    raw_output_path = OUTPUT_DIR / 'raw_scan_results.json'
    with open(raw_output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    console.print(f"[green]–ù–µ–æ—á–∏—â–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª {raw_output_path}[/green]")
    finder = DuplicateFinder()
    cleaned_results = finder.clean_duplicates(results)
    cleaned_output_path = OUTPUT_DIR / 'scan_results.json'
    with open(cleaned_output_path, 'w', encoding='utf-8') as f:
        json.dump(cleaned_results, f, ensure_ascii=False, indent=2)
    console.print(f"[green]–û—á–∏—â–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª {cleaned_output_path}[/green]")
    display_results_optimized(cleaned_results)
    elapsed_time = time.perf_counter() - start_time
    console.print("\n" + "=" * 60)
    console.print(f"[cyan]–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:[/cyan]")
    console.print(f"[green]–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤:[/green] {len(files)}")
    console.print(f"[green]–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–¥–æ –æ—á–∏—Å—Ç–∫–∏):[/green] {len(results)}")
    console.print(f"[green]–ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏):[/green] {len(cleaned_results)}")
    console.print(f"[green]–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:[/green] {elapsed_time:.2f} —Å–µ–∫.")
    console.print("=" * 60 + "\n")

def main():
    console.print(BANNER)
    console.print("\n")
    asyncio.run(main_async())

if __name__ == "__main__":
    main()