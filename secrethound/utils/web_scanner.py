import asyncio
import aiohttp
import aiofiles
import re
import os
from pathlib import Path
from urllib.parse import urljoin, urlparse
from typing import Set, List, Optional
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn

console = Console()

class WebScanner:
    """
    –°–∫–∞–Ω–µ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–π–ª–æ–≤ —Å –≤–µ–±-—Å–µ—Ä–≤–∏—Å–æ–≤
    """
    
    def __init__(self, max_depth: int = 2, max_file_size: int = 1024 * 1024):
        self.max_depth = max_depth
        self.max_file_size = max_file_size
        self.session: Optional[aiohttp.ClientSession] = None
        self.visited_urls: Set[str] = set()
        self.downloaded_files: List[Path] = []
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
        self.target_extensions = {
            '.js', '.ts', '.jsx', '.tsx', '.json', '.xml', 
            '.html', '.htm', '.css', '.txt', '.md', '.yaml', '.yml'
        }
        
        # CDN –¥–æ–º–µ–Ω—ã –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        self.cdn_domains = {
            'cdnjs.cloudflare.com', 'unpkg.com', 'jsdelivr.net',
            'code.jquery.com', 'cdn.jsdelivr.net', 'stackpath.bootstrapcdn.com'
        }
    
    async def __aenter__(self):
        # –°–æ–∑–¥–∞–µ–º SSL –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –æ—à–∏–±–∫–∏ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–≤
        import ssl
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        connector = aiohttp.TCPConnector(ssl=ssl_context)
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': 'SecretHound/1.0'},
            connector=connector
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def _should_skip_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å URL"""
        parsed = urlparse(url)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º CDN
        if any(cdn in parsed.netloc for cdn in self.cdn_domains):
            return True
            
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –ø–æ—Å–µ—â–µ–Ω–Ω—ã–µ
        if url in self.visited_urls:
            return True
            
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ-HTTP(S)
        if parsed.scheme not in ('http', 'https'):
            return True
            
        return False
    
    def _get_filename_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL"""
        parsed = urlparse(url)
        path = parsed.path
        
        # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ –ø—É—Ç–∏
        filename = path.split('/')[-1]
        
        # –ï—Å–ª–∏ –∏–º—è —Ñ–∞–π–ª–∞ –ø—É—Å—Ç–æ–µ –∏–ª–∏ –Ω–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º
        if not filename or '.' not in filename:
            content_type = 'text/plain'  # –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–∑–∂–µ
            ext = self._get_file_extension(url, content_type)
            filename = f"file_{len(self.downloaded_files):04d}{ext}"
        
        return filename
    
    def _get_file_extension(self, url: str, content_type: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        parsed = urlparse(url)
        path = parsed.path.lower()
        
        # –ò–∑ URL
        for ext in self.target_extensions:
            if path.endswith(ext):
                return ext
        
        # –ò–∑ Content-Type
        if 'javascript' in content_type:
            return '.js'
        elif 'json' in content_type:
            return '.json'
        elif 'html' in content_type:
            return '.html'
        elif 'css' in content_type:
            return '.css'
        elif 'xml' in content_type:
            return '.xml'
        
        return '.txt'
    
    async def _download_file(self, url: str, output_dir: Path) -> Optional[Path]:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª"""
        try:
            async with self.session.get(url) as response:
                if response.status != 200:
                    return None
                
                content_type = response.headers.get('content-type', '').lower()
                content_length = int(response.headers.get('content-length', 0))
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä
                if content_length > self.max_file_size:
                    console.print(f"[yellow]–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª: {url} ({content_length} bytes)[/yellow]")
                    return None
                
                # –ß–∏—Ç–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                content = await response.read()
                
                # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
                original_filename = self._get_filename_from_url(url)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ñ–∞–π–ª —Å —Ç–∞–∫–∏–º –∏–º–µ–Ω–µ–º
                file_path = output_dir / original_filename
                counter = 1
                while file_path.exists():
                    name, ext = os.path.splitext(original_filename)
                    file_path = output_dir / f"{name}_{counter}{ext}"
                    counter += 1
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
                async with aiofiles.open(file_path, 'wb') as f:
                    await f.write(content)
                
                self.downloaded_files.append(file_path)
                console.print(f"[green]‚úì –°–∫–∞—á–∞–Ω: {url} -> {file_path.name}[/green]")
                return file_path
                
        except Exception as e:
            console.print(f"[red]‚úó –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}[/red]")
            return None
    
    async def _extract_links(self, html_content: str, base_url: str) -> Set[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –∏–∑ HTML"""
        links = set()
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ñ–∞–π–ª—ã
        js_patterns = [
            r'src=["\']([^"\']*\.js[^"\']*)["\']',
            r'href=["\']([^"\']*\.css[^"\']*)["\']',
            r'url\(["\']?([^"\')\s]*\.(?:js|css|json|xml|yaml|yml)["\']?\)',
        ]
        
        for pattern in js_patterns:
            try:
                matches = re.findall(pattern, html_content, re.IGNORECASE)
                for match in matches:
                    full_url = urljoin(base_url, match)
                    if not self._should_skip_url(full_url):
                        links.add(full_url)
            except re.error as e:
                console.print(f"[yellow]–û—à–∏–±–∫–∞ –≤ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–º –≤—ã—Ä–∞–∂–µ–Ω–∏–∏: {e}[/yellow]")
        
        return links
    
    async def scan_website(self, base_url: str, output_dir: Path) -> List[Path]:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç –≤–µ–±-—Å–∞–π—Ç –∏ —Å–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª—ã"""
        console.print(f"[cyan]üîç –ù–∞—á–∏–Ω–∞—é —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ: {base_url}[/cyan]")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–º–µ–Ω –∏–∑ URL
        parsed_url = urlparse(base_url)
        domain = parsed_url.netloc
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É —Å –¥–æ–º–µ–Ω–æ–º –≤–Ω—É—Ç—Ä–∏ web_files
        domain_dir = output_dir / domain
        domain_dir.mkdir(parents=True, exist_ok=True)
        
        console.print(f"[cyan]üìÅ –§–∞–π–ª—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {domain_dir}[/cyan]")
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        try:
            async with self.session.get(base_url) as response:
                if response.status == 200:
                    html_content = await response.text()
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                    main_file = domain_dir / "index.html"
                    async with aiofiles.open(main_file, 'w', encoding='utf-8') as f:
                        await f.write(html_content)
                    self.downloaded_files.append(main_file)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏
                    links = await self._extract_links(html_content, base_url)
                    
                    # –°–∫–∞—á–∏–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
                    with Progress(
                        SpinnerColumn(),
                        TextColumn("[progress.description]{task.description}"),
                        BarColumn(),
                        TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                        console=console
                    ) as progress:
                        task = progress.add_task("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤...", total=len(links))
                        
                        for link in links:
                            await self._download_file(link, domain_dir)
                            progress.advance(task)
                            
        except Exception as e:
            console.print(f"[red]–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ {base_url}: {e}[/red]")
        
        console.print(f"[green]‚úÖ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°–∫–∞—á–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(self.downloaded_files)}[/green]")
        return self.downloaded_files

async def download_and_scan_website(url: str, output_dir: Path) -> List[Path]:
    """–£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤–µ–±-—Å–∞–π—Ç–∞"""
    async with WebScanner() as scanner:
        return await scanner.scan_website(url, output_dir) 